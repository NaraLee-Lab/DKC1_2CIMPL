{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKC1 2CIMPL EBER2 Extraction\n",
    "> This pipeline is designed to extract and analyze regions of the human genome identified by [2CIMPL](https://pubmed.ncbi.nlm.nih.gov/32610124/) as interacting with the EBV-encoded RNA 2 ([EBER2](https://pubmed.ncbi.nlm.nih.gov/26951683/)) region of the Epstein-Barr Virus genome.\n",
    "\n",
    "### Software Dependencies:\n",
    "* Python3\n",
    "* Perl (used for CTK/PCR Collapsing)\n",
    "* [CTK](https://github.com/chaolinzhanglab/ctk) - PCR Collapsing\n",
    "* FastQC\n",
    "* SAMtools\n",
    "* BEDtools\n",
    "* Bowtie2\n",
    "\n",
    "### Python Packages:\n",
    "* BioPython\n",
    "* PySAM\n",
    "* datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pysam\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "* genome_files - [bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#the-bowtie2-build-indexer) genome files\n",
    "* software_dir - CTK directory (fastq2collapse.pl)\n",
    "* in_file - input FASTQ\n",
    "* output_dir - default is current directory (all necessary output directories are generated on execution)\n",
    "* bt2 threads - default is 16 ___(CHANGE THIS)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2971181648.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [2]\u001b[0;36m\u001b[0m\n\u001b[0;31m    output_dir   = .\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Update these as necessary\n",
    "genome_files = ('path/to/genome/genome_prefix')\n",
    "software_dir = ('.../path/to/ctk')\n",
    "in_file      = ('.../path/to/fastq')\n",
    "output_dir   = ('.')\n",
    "\n",
    "## Double-check the number of threads available, the bt2 parameters in this notebook are tailored for cluster computing!\n",
    "bt2_threads  = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastQC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $output_dir/fastqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fastqc -o $output_dir/fastqc --no-extract $in_file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCR Collapse:\n",
    "> This Perl script is provided through the CLIP Toolkt (CTK) and is used to collapse PCR duplicates and greatly reduce the size of input files. This step is crucial to the efficient performance of this pipeline, and can reduce the original sanple size by around 60% in some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -v makes it verbose\n",
    "!perl $software_dir/fastq2collapse.pl -v $in_file $output_dir/collapsed.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align with Bowtie2:\n",
    "> The parameters used in this alignment are identical to those used in [Aligater's](https://github.com/timbitz/Aligater) native \"align\" function, which makes use of bowtie2 in a SAMtools-friendly wrapper. We used these parameters for the sake of continuity, as we had run previous analyses using aligater as well as much more relaxed bt2 parameters, and these parameters generated the highest number of identified EBER2 hybrids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $output_dir/bowtie2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feel free to play around with the quality threshold and k value, but when we ran this analysis with -k = 100 there were no changes\n",
    "!bowtie2 -p $bt2_threads -k 50 -R 3 -N 0 -L 16 -i S,1,0.50 --local --reorder -x $genome_files \\\n",
    "-U $output_dir/collapsed.fastq 1> $output_dir/bowtie2/aligned.sam 2> $output_dir/bowtie2/bt2_align.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check BT2 output:\n",
    "* \"cat\" the log file\n",
    "* Should give the alignment rate, number of reads, and percentage of multi-mapped reads, as well as any errors thrown.\n",
    "    * The percentage of multi-mapped reads should be much higher than single-mapped, since 2CIMPL generates hybrids through UV-crosslinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $output_dir/bowtie2/bt2_align.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM Extraction & Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $output_dir/extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to BAM and sort\n",
    "!samtools view -bS $output_dir/bowtie2/aligned.sam | samtools sort > $output_dir/extract/aligned.bam\n",
    "## Index the BAM output\n",
    "!samtools index $output_dir/extract/aligned.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting EBER2-specific reads:\n",
    "* Use the samtools view -b function to extract the reads that mapped to chrEBV (or your chromosome of interest)\n",
    "* Reads within a given range on said chromosome will have their IDs extracted & written to a text file, done by \"awk-ing\" the 4th column (start-of-alignment field for SAM files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the reads mapping to chrEBV, and select those reads that fall within our specified range\n",
    "!samtools view -b $output_dir/extract/aligned.bam chrEBV | samtools view -h \\\n",
    "| awk '$4 >= 6961 && $4 <= 7135'> $output_dir/extract/EBER2_extracted.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a SAM header (-H) to the hybrid SAM file\n",
    "!samtools view -H $output_dir/bowtie2/aligned.sam > $output_dir/bowtie2/aligned_header.sam\n",
    "!cat $output_dir/bowtie2/aligned_header.sam $output_dir/extract/EBER2_extracted.sam > $output_dir/extract/EBER2_headed.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting EBER2-associated read IDs:\n",
    "> In order to search the SAM file for hybrids, we first must extract the IDs. The theory is this: when 2CIMPL generates hybrid reads through UV crosslinking, a portion of each of the initial sequences will be maintained and subsequently align to their respective portions of the genome. With the knowledge that we are looking for EBER2-associated hybrids, if we identify all hybrids that mapped to EBER2 and some other portion of the genome, we can limit the field of search substantially. So, we just search the bowtie2 output file for our selected IDs and generate a new SAM file comprised of all EBER2 hybrid reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the first field (Read ID) and write the unqiue reads (no duplicates) to a text file\n",
    "!samtools view -h $output_dir/extract/EBER2_headed.sam | cut -f 1 | sort | uniq > $output_dir/extract/EBER2_ids.txt\n",
    "# Count the number of extracted IDs:\n",
    "!wc -l $output_dir/extract/EBER2_ids.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Hybrids from SAM and FASTQ files:\n",
    "> This code is fairly optimal for searching decently large FASTQ and SAM files, but for very large files it is best to run the following chunks on a computing cluster where available. We were able to do this by initiating a Conda environment and importing the necessary methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting from FASTQ:\n",
    "> Check each record in the FASTQ, see if it's in our set of wanted IDs. If it is, write to an output FASTQ. This is a handy way to cross-reference the mapping quality and sequences, especially when identifying which section of the hybrid is associated to the aligned region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = output_dir+\"/extract/EBER2_ids.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastq_extract(fastq_in, fastq_out, ids):\n",
    "    \n",
    "    # Generate a set of IDs to facillitate faster searching as compared to a list\n",
    "    wanted_ids = set(x[:-1] for x in open(ids))\n",
    "    \n",
    "    fastq_start = datetime.now()\n",
    "    \n",
    "    output_handle = open(fastq_out, \"w\")\n",
    "\n",
    "    wanted = (record for record in SeqIO.parse(in_file,\"fastq\") if record.id in wanted_ids)\n",
    "    SeqIO.write(wanted, output_handle, \"fastq\")\n",
    "    output_handle.close()\n",
    "\n",
    "    fastq_end = datetime.now()-fastq_start\n",
    "    print(\"total FASTQ extraction time: \", fastq_end.seconds/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract reads from initial FASTQ file\n",
    "fastq_in  = in_file\n",
    "fastq_out = output_dir+\"/extract/EBER2_hybrids.fastq\"\n",
    "\n",
    "fastq_extract(fastq_in, fastq_out, ids)\n",
    "\n",
    "## Count number of reads in output FASTQ file\n",
    "!awk '{s++}END{print s/4}' $fastq_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting from SAM:\n",
    "> Input is a BAM file, output is a SAM file. PySam's fetch iterator is very quick, which is why it was chosen for this application. The read IDs are read into a map rather than a set in this application, which performed better when coupled with PySam fetch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_extract(ibam, osam, ids):\n",
    "    sam_start = datetime.now()\n",
    "\n",
    "    qnames = set(map(str.strip, open(ids)))\n",
    "    bam_in = pysam.AlignmentFile(ibam)\n",
    "    sam_out = pysam.AlignmentFile(osam, \"w\", template=bam)\n",
    "\n",
    "    for b in bam_in.fetch(until_eof=True):\n",
    "        if b.query_name in qnames:\n",
    "            sam_out.write(b)\n",
    "    bam_in.close()\n",
    "    sam_out.close()\n",
    "\n",
    "    sam_end = datetime.now()-sam_start\n",
    "\n",
    "    print(\"total SAM extraction time: \",sam_end.seconds/60,\" minutes\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibam = output_dir+\"/extract/aligned.bam\"\n",
    "osam = output_dir+\"/extract/EBER2_hybrids.sam\"\n",
    "\n",
    "sam_extract(ibam, osam, ids)\n",
    "\n",
    "## Count number of reads in SAM file\n",
    "count = !wc -l $osam\n",
    "int((str(count).split())[1])-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Bedgraph:\n",
    "> For post-processing, it's best to open the bedgraph in Excel and format it as a table. The result is a more permanent record that's human readable and can be readily sorted and re-sorted, which is why we avoided generating  and exporting a dataframe within Jupyter. For this analysis, we added an additional column to the Excel sheet documenting the mapping postion in the human genome and all associated genes/snoRNA identified through UCSC GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!samtools view -bS $output_dir/extract/EBER2_hybrids.sam > $output_dir/extract/EBER2_hybrids.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bedtools genomecov -ibam $output_dir/extract/EBER2_hybrids.bam -bg | awk '$1 != \"chrEBV\"' > $output_dir/extract/EBER2_hybrids.bedgraph"
   ]
  }
 ],
 "metadata": {
  "author": {
   "email": "bts55@pitt.edu",
   "name": "Brent Schlegel"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "title": "DKC1_2CIMPL_EBER2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
